{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIS 678 Project 4 pt. 2  \n",
    "## Backpropagation for Neural Network Learning\n",
    "### By Russell Marvin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions will include feedforward(), backprop(), and learn(). Feedforward will feed inputs through the network, backprop will backpropagate error through the network, and learn will update weights accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#defining the initial input values i1 and i2 [i1,i2]\n",
    "input = np.array([0,1])\n",
    "\n",
    "#first layer weights in the format [[wi1h1,wi1h2],[wi2h1,wi2h2]]\n",
    "weights1 = np.array([[1,-1],[.5,2]])\n",
    "\n",
    "#first layer bias weights in format [wb0h1,wb0h2]\n",
    "biases1 = np.array([1,1])\n",
    "\n",
    "#second layer initial weights in format [wh1y,h2y]\n",
    "weights2 = np.array([1.5,-1])\n",
    "\n",
    "#second layer bias weight in format [wb1y]\n",
    "biases2 = np.array([1])\n",
    "\n",
    "def feedforward(inputs,weights1,weights2,biases1,biases2):\n",
    "    print('Step 1: Feed the inputs forward. \\n')\n",
    "    #sum of products + bias for each hidden unit (h1 and h2)\n",
    "    sigma1 = np.dot(inputs,weights1) + biases1\n",
    "    print(sigma1,'sigma, aka sum of products for hidden units h1 and h2.')\n",
    "    #sigmoid for both hidden units (h1 and h2) to calculate y\n",
    "    hi = (1/(1+np.exp(-sigma1)))\n",
    "    print(hi,'hi, aka the outputs of hidden units h1 and h2')\n",
    "    sigma2 = np.dot(hi,weights2) + biases2\n",
    "    print(sigma2,'sigma, aka sum of products for output unit y.')\n",
    "    #sigmoid\n",
    "    y = (1/(1+np.exp(-sigma2)))\n",
    "    print(y,'y, aka output of the network')\n",
    "    #target = 1 for this example, t could be assigned as a variable\n",
    "    total_error = (.5)*((1-y)**2)\n",
    "    print(total_error,'total error in the network.\\n')\n",
    "    print('Step 2: Backpropagate the error.\\n')\n",
    "    return y,hi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first function `feedforward()` uses `numpy.dot()` [(docs) ](https://numpy.org/doc/stable/reference/generated/numpy.dot.html)for matrix multiplication of the input values and weights. It also uses `numpy.exp()` to employ the sigmoid function (on `sigma1` and `sigma2`) by . \n",
    "\n",
    "The function takes inputs, weights (for both layers of edges) and bias weights (again for both layers of edges). It then applies matrix multiplication of inputs and weights, adding biases (calculating sum of products) for each of the hidden units, then applies the sigmoid activation function. It then takes those two outputs of the hidden units as inputs and uses `numpy.dot()` again with the second layer weights, adding biases as well for a second sum of products calculation. Finally, `y` is calculated by applying the sigmoid function to this sum of products `sigma2`. We are able to calculate the total error in the network here using y (and target t, which is 1 in this network.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(input, weights1,weights2,biases1,biases2):\n",
    "    #load in the data from feedforward (step 1)\n",
    "    data = feedforward(input,weights1,weights2,biases1,biases2)\n",
    "    y = data[0]\n",
    "    hi = data[1]\n",
    "    output_error = (y)*(1-y)*(1-y)\n",
    "    print(output_error,'Ey aka the error in the output unit y')\n",
    "    #saving hidden unit weight errors in a list\n",
    "    hi_errors = []\n",
    "    #now using a loop for error (I got confused on how to \n",
    "    # do this calculation with np.dot)\n",
    "    for i in range(len(hi)):\n",
    "        Ehi = (hi[i])*(1-(hi[i]))*(weights2[i]*output_error)\n",
    "        #print(Ehi)\n",
    "        hi_errors.append(Ehi)\n",
    "    print(hi_errors,'hi_errors or Ehi, aka the errors in h1 and h2.\\n')\n",
    "    return hi_errors,hi,output_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `backprop()` function takes input, weights and biases like the other functions and returns a set of errors for h1 and h2 (`hi_errors`), as well as the values for h1 and h2 (`hi`), and the error of the output unit y (`output_error`). \n",
    "\n",
    "This function takes an algorithmic approach by looping through the values of `hi`, which are the outputs of the hidden units, and calculating their error values. It then adds them to a list `hi_errors` which is sent through to the next function, learn, along with `hi` and `output_error` (described above). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Feed the inputs forward. \n",
      "\n",
      "[1.5 3. ] sigma, aka sum of products for hidden units h1 and h2.\n",
      "[0.81757448 0.95257413] hi, aka the outputs of hidden units h1 and h2\n",
      "[1.27378759] sigma, aka sum of products for output unit y.\n",
      "[0.78139043] y, aka output of the network\n",
      "[0.02389507] total error in the network.\n",
      "\n",
      "Step 2: Backpropagate the error.\n",
      "\n",
      "[0.03734276] Ey aka the error in the output unit y\n",
      "[array([0.00835431]), array([-0.00168702])] hi_errors or Ehi, aka the errors in h1 and h2.\n",
      "\n",
      "Step 3: Learn (update weights proportionally).\n",
      "\n",
      "New weights, in the format wab where w is the weight on the edge from unit a to unit b: {'wh1,y': 1.5153, 'wh2,y': -0.9822, 'wb1,y': 1.0187, 'wi1,h1': 1.0, 'wi1,h2': -1.0, 'wi2,h1': 0.5042, 'wi2,h2': 1.9992, 'wb0,h1': 1.0042, 'wb0,h2': 0.9992}\n"
     ]
    }
   ],
   "source": [
    "def learn(input,weights1,weights2,biases1,biases2):\n",
    "    #call backprop for the hidden unit errors 'hi_errors',\n",
    "    #hidden unit outputs 'hi' and the error of output unit y 'output_error'\n",
    "    data = backprop(input,weights1,weights2,biases1,biases2)\n",
    "    hi_errors = data[0]\n",
    "    hi = data[1]\n",
    "    output_error = data[2]\n",
    "    #saving new weights in dictionary\n",
    "    new_weights = {}\n",
    "    #loop through weights2 (second layer weights)\n",
    "    for i in range(len(weights2)):\n",
    "        new_weight = weights2[i] + ((.5)*(output_error)*(hi[i]))\n",
    "        new_weights[f'wh{i+1},y'] = round(float(new_weight),4)\n",
    "    #loop through biases (second layer bias), which in this case is just one bias\n",
    "    #weight b/c there is only one output unit\n",
    "    for i in range(len(biases2)):\n",
    "        #output (z) for this weight is 1 b/c bias outputs are 1\n",
    "        new_weight = biases2[i] + ((.5)*(output_error)*(1))\n",
    "        new_weights[f'wb{i+1},y'] = round(float(new_weight),4)\n",
    "    #loop through as many values as there are in weights1\n",
    "    for i in range(len(weights1)):\n",
    "        #loop through each element in the i items in weights1\n",
    "        for j in range(len(weights1[i])):\n",
    "            #apply learn rate and update weights using error (hi_errors)\n",
    "            new_weight = weights1[i][j] + (.5)*(hi_errors[j])*(input[i])\n",
    "            #add it to our dictionary\n",
    "            new_weights[f'wi{i+1},h{j+1}'] = round(float(new_weight),4)\n",
    "    #loop through values in first layer bias weights\n",
    "    for i in range(len(biases1)):\n",
    "        #output (z) for this weight is 1 b/c bias outputs are 1\n",
    "        new_weight = biases1[i] + ((.5)*(hi_errors[i])*(1))\n",
    "        #these weights run from bias b0 to the hidden units h1 and h2\n",
    "        #I use i+1 to index the hidden units based on their order\n",
    "        new_weights[f'wb{0},h{i+1}'] = round(float(new_weight),4)\n",
    "    print('Step 3: Learn (update weights proportionally).\\n')\n",
    "    print('New weights, in the format wab where w is the weight on the edge from unit a to unit b:',new_weights)\n",
    "\n",
    "learn(input,weights1,weights2,biases1,biases2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `learn()` function takes the same parameters as the others, calling `backprop()` using those parameters, which then calls `feedforward()` with those same parameters. This way the starting points given for our network can be fed in (input values i1 and i2, bias weights from nodes b0 and b1, and weights from i1 and i2 as well as h1 and h2. )\n",
    "\n",
    "The output of the `learn()` function is in the format of a dictionary where the key is the weight name and the value is the value representing the updated weight. These weights have the format w<sub>ab </sub> the weight w is on the edge from node a to node b. For example the weight on the edge between hidden unit 1 and y (the final output node) is written as w<sub>h1y </sub>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "552eff63ae26b1f7715ef9da8940d59e9923cc9f3696b79d9372b91e56e1a4b8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
